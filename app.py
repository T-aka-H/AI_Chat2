import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from typing import List, Tuple, Dict
import os

# è¨­å®š
st.set_page_config(
    page_title="å¤§è°·ç¿”å¹³ AI Chat - 3å±¤æ®µéšRAGã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="âš¾",
    layout="wide"
)

class OhtaniRAGSystem:
    def __init__(self, csv_path: str):
        """å¤§è°·ç¿”å¹³RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.df = pd.read_csv(csv_path)
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        self.question_vectors = self.vectorizer.fit_transform(self.df['Question'])
        self.answer_vectors = TfidfVectorizer(max_features=1000).fit_transform(self.df['Answer'])
        
        # å¤§è°·é¸æ‰‹ã®ç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        self.speech_patterns = self._extract_speech_patterns()
        
    def _extract_speech_patterns(self) -> List[str]:
        """å¤§è°·é¸æ‰‹ã®å®Ÿéš›ã®ç™ºè¨€ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        patterns = []
        
        # å›ç­”ã‹ã‚‰ã‚ˆãä½¿ã‚ã‚Œã‚‹è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        answers = self.df['Answer'].tolist()
        
        # å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        common_patterns = [
            r'ã¾ã ã¾ã .*?ã¨æ€ã„ã¾ã™',
            r'.*?ã®ãŠã‹ã’ã§.*?',
            r'.*?ã‹ã‚‰å­¦ã¶ã“ã¨ãŒ.*?',
            r'ãã†ã§ã™ã­.*?',
            r'.*?ã ã¨æ€ã†ã®ã§.*?',
            r'.*?ã¨ã„ã†ã®ã¯.*?',
            r'.*?ã‹ãªã¨æ€ã„ã¾ã™',
            r'.*?ã˜ã‚ƒãªã„ã‹ãªã¨.*?',
            r'.*?ã—ã¦ã„ããŸã„ãªã¨æ€ã„ã¾ã™',
            r'.*?ã¨ã„ã†ã“ã¨ã§ã™',
        ]
        
        for answer in answers:
            for pattern in common_patterns:
                matches = re.findall(pattern, answer)
                patterns.extend(matches[:3])  # æœ€å¤§3å€‹ã¾ã§
        
        return list(set(patterns))[:50]  # é‡è¤‡é™¤å»ã—ã¦50å€‹ã¾ã§
    
    def layer1_direct_search(self, query: str, threshold: float = 0.7) -> Tuple[List[Dict], str]:
        """Layer 1: ç›´æ¥æ¤œç´¢ (é–¾å€¤0.7) â†’ ä¿¡é ¼åº¦: é«˜"""
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.question_vectors)[0]
        
        best_idx = np.argmax(similarities)
        best_score = similarities[best_idx]
        
        if best_score >= threshold:
            result = [{
                'question': self.df.iloc[best_idx]['Question'],
                'answer': self.df.iloc[best_idx]['Answer'],
                'score': best_score,
                'id': self.df.iloc[best_idx]['ID'],
                'confidence': 'high'
            }]
            return result, "ç›´æ¥æ¤œç´¢ã§é«˜ç²¾åº¦ãƒãƒƒãƒã‚’ç™ºè¦‹"
        
        return [], "ç›´æ¥æ¤œç´¢ã§ã¯è©²å½“ãªã—"
    
    def layer2_concept_search(self, query: str, threshold: float = 0.5) -> Tuple[List[Dict], str]:
        """Layer 2: æ¦‚å¿µæ¤œç´¢ (é–¾å€¤0.5) â†’ ä¿¡é ¼åº¦: ä¸­"""
        
        # æ¦‚å¿µæ‹¡å¼µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        concept_expansions = {
            'AI': ['æŠ€è¡“', 'æ–°ã—ã„', 'å­¦ç¿’', 'é€²æ­©', 'æŒ‘æˆ¦', 'æœªæ¥'],
            'å®‡å®™': ['å¤¢', 'æŒ‘æˆ¦', 'æ–°ã—ã„', 'ç›®æ¨™', 'åºƒã„'],
            'æ–™ç†': ['é£Ÿäº‹', 'å¥½ã', 'æ¥½ã—ã„', 'ãŠã„ã—ã„'],
            'æ˜ ç”»': ['è¦‹ã‚‹', 'æ¥½ã—ã„', 'æ™‚é–“', 'ãƒªãƒ©ãƒƒã‚¯ã‚¹'],
            'éŸ³æ¥½': ['èã', 'å¥½ã', 'ãƒªãƒ©ãƒƒã‚¯ã‚¹', 'æ¥½ã—ã„'],
            'å®¶æ—': ['å¤§åˆ‡', 'æ”¯ãˆ', 'ã‚ã‚ŠãŒãŸã„', 'æ„Ÿè¬'],
            'å‹é”': ['ä»²é–“', 'ãƒãƒ¼ãƒ ãƒ¡ãƒ¼ãƒˆ', 'å¤§åˆ‡', 'ä¿¡é ¼'],
            'å°†æ¥': ['ç›®æ¨™', 'å¤¢', 'æŒ‘æˆ¦', 'é ‘å¼µã‚‹'],
            'å›°é›£': ['æŒ‘æˆ¦', 'ä¹—ã‚Šè¶Šãˆã‚‹', 'å­¦ã¶', 'æˆé•·'],
            'æˆåŠŸ': ['åŠªåŠ›', 'ç¶™ç¶š', 'ãƒãƒ¼ãƒ ', 'æ„Ÿè¬'],
        }
        
        # ã‚¯ã‚¨ãƒªã‹ã‚‰æ¦‚å¿µã‚’æŠ½å‡ºã—ã¦æ‹¡å¼µ
        expanded_query = query
        for concept, keywords in concept_expansions.items():
            if concept in query:
                expanded_query += ' ' + ' '.join(keywords)
        
        # æ‹¡å¼µã‚¯ã‚¨ãƒªã§æ¤œç´¢
        query_vector = self.vectorizer.transform([expanded_query])
        similarities = cosine_similarity(query_vector, self.question_vectors)[0]
        
        # ä¸Šä½3ä»¶ã‚’å–å¾—
        top_indices = np.argsort(similarities)[-3:][::-1]
        results = []
        
        for idx in top_indices:
            if similarities[idx] >= threshold:
                results.append({
                    'question': self.df.iloc[idx]['Question'],
                    'answer': self.df.iloc[idx]['Answer'],
                    'score': similarities[idx],
                    'id': self.df.iloc[idx]['ID'],
                    'confidence': 'medium'
                })
        
        if results:
            return results, f"æ¦‚å¿µæ¤œç´¢ã§{len(results)}ä»¶ç™ºè¦‹ (æ‹¡å¼µ: {expanded_query})"
        
        return [], "æ¦‚å¿µæ¤œç´¢ã§ã‚‚è©²å½“ãªã—"
    
    def layer3_pattern_generation(self, query: str) -> Tuple[str, str]:
        """Layer 3: ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ â†’ ä¿¡é ¼åº¦: ä½"""
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ
        import random
        selected_patterns = random.sample(self.speech_patterns, min(3, len(self.speech_patterns)))
        
        # é–¢é€£ã™ã‚‹å›ç­”ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.answer_vectors)[0]
        top_answer_idx = np.argmax(similarities)
        related_answer = self.df.iloc[top_answer_idx]['Answer']
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®å›ç­”ç”Ÿæˆ
        generated_response = self._generate_pattern_response(query, selected_patterns, related_answer)
        
        return generated_response, f"ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ (ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(selected_patterns)}å€‹)"
    
    def _generate_pattern_response(self, query: str, patterns: List[str], related_answer: str) -> str:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®å›ç­”ç”Ÿæˆ"""
        
        # åŸºæœ¬çš„ãªå¿œç­”ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        base_responses = [
            f"ãã†ã§ã™ã­ã€{query}ã«ã¤ã„ã¦ã¯ã€ã¾ã ã¾ã å‹‰å¼·ä¸è¶³ã§ã™ãŒ",
            f"{query}ã«é–¢ã—ã¦ã¯ã€",
            f"ã†ãƒ¼ã‚“ã€{query}ã¨ã„ã†ã®ã¯",
        ]
        
        # é–¢é€£å›ç­”ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = ['æŒ‘æˆ¦', 'å­¦ã¶', 'å¤§åˆ‡', 'åŠªåŠ›', 'ç¶™ç¶š', 'æ„Ÿè¬', 'ãƒãƒ¼ãƒ ']
        
        import random
        base = random.choice(base_responses)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰è¦ç´ ã‚’çµ„ã¿åˆã‚ã›
        middle_parts = [
            "æ–°ã—ã„ã“ã¨ã«æŒ‘æˆ¦ã™ã‚‹ã®ã¯ç´ æ™´ã‚‰ã—ã„ã“ã¨ã ã¨æ€ã„ã¾ã™",
            "ã¾ã ã¾ã å­¦ã¶ã“ã¨ãŒãŸãã•ã‚“ã‚ã‚‹ã¨æ„Ÿã˜ã¦ã„ã¾ã™",
            "ã“ã‚Œã‹ã‚‰ã‚‚åŠªåŠ›ã‚’ç¶šã‘ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™",
            "ãƒãƒ¼ãƒ ã®ã¿ã‚“ãªã®ãŠã‹ã’ã§æˆé•·ã§ãã¦ã„ã¾ã™"
        ]
        
        ending_parts = [
            "é‡çƒä»¥å¤–ã®ã“ã¨ã‹ã‚‰ã‚‚å­¦ã¶ã“ã¨ãŒãŸãã•ã‚“ã‚ã‚‹ã¨æ€ã£ã¦ã„ã¾ã™ã€‚",
            "ã“ã‚Œã‹ã‚‰ã‚‚é ‘å¼µã£ã¦ã„ããŸã„ãªã¨æ€ã„ã¾ã™ã€‚",
            "ã¾ã ã¾ã æœªç†Ÿã§ã™ãŒã€ç¶™ç¶šã—ã¦ã„ãã“ã¨ãŒå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚"
        ]
        
        response = base + random.choice(middle_parts) + "ã€‚" + random.choice(ending_parts)
        
        return response
    
    def search(self, query: str) -> Dict:
        """3å±¤æ®µéšæ¤œç´¢ã®å®Ÿè¡Œ"""
        
        # Layer 1: ç›´æ¥æ¤œç´¢
        results1, msg1 = self.layer1_direct_search(query)
        if results1:
            return {
                'layer': 1,
                'results': results1,
                'message': msg1,
                'confidence': 'high',
                'response': results1[0]['answer'],
                'source': f"ID {results1[0]['id']}: {results1[0]['question']}"
            }
        
        # Layer 2: æ¦‚å¿µæ¤œç´¢
        results2, msg2 = self.layer2_concept_search(query)
        if results2:
            # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„å›ç­”ã‚’é¸æŠ
            best_result = results2[0]
            return {
                'layer': 2,
                'results': results2,
                'message': msg2,
                'confidence': 'medium',
                'response': best_result['answer'],
                'source': f"ID {best_result['id']}: {best_result['question']}"
            }
        
        # Layer 3: ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
        generated_response, msg3 = self.layer3_pattern_generation(query)
        return {
            'layer': 3,
            'results': [],
            'message': msg3,
            'confidence': 'low',
            'response': generated_response,
            'source': "å¤§è°·é¸æ‰‹ã®ç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ç”Ÿæˆ"
        }

def main():
    st.title("âš¾ å¤§è°·ç¿”å¹³ AI Chat")
    st.subheader("ğŸš€ 3å±¤æ®µéšRAGã‚·ã‚¹ãƒ†ãƒ  - çœŸã®RAGä¾¡å€¤ã‚’ç™ºæ®ã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚·ã‚¹ãƒ†ãƒ èª¬æ˜
    with st.expander("ğŸ”¥ ã‚·ã‚¹ãƒ†ãƒ ã®é©æ–°çš„ç‰¹å¾´"):
        st.markdown("""
        ### 3å±¤æ®µéšæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
        - **Layer 1: ç›´æ¥æ¤œç´¢** (é–¾å€¤0.7) â†’ ä¿¡é ¼åº¦: **é«˜** ğŸ”´
        - **Layer 2: æ¦‚å¿µæ¤œç´¢** (é–¾å€¤0.5) â†’ ä¿¡é ¼åº¦: **ä¸­** ğŸŸ¡  
        - **Layer 3: ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ** â†’ ä¿¡é ¼åº¦: **ä½** ğŸŸ¢
        
        ### ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ ã®ä¾¡å€¤
        - **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å®Œå…¨æ´»ç”¨** - ã©ã‚“ãªè³ªå•ã§ã‚‚é–¢é€£æƒ…å ±ã‚’ç™ºè¦‹
        - **å®Ÿéš›ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ä½¿ç”¨** - ç”ŸæˆAIã¸ã®æŒ‡ç¤ºã§ã¯ãªãã€å®Ÿãƒ‡ãƒ¼ã‚¿åŸºæº–
        - **æ®µéšçš„ä¿¡é ¼åº¦** - ã©ã®å±¤ã‚’ä½¿ç”¨ã—ãŸã‹ã§ä¿¡é ¼åº¦ã‚’æ˜ç¤º
        - **æ ¹æ‹ ã®é€æ˜æ€§** - å‚è€ƒã«ã—ãŸå®Ÿéš›ã®ç™ºè¨€ã‚’è¡¨ç¤º
        """)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    @st.cache_resource
    def load_rag_system():
        return OhtaniRAGSystem('ohtani_rag_final.csv')
    
    try:
        rag_system = load_rag_system()
        st.success(f"âœ… RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº† ({len(rag_system.df)}å€‹ã®QAãƒšã‚¢)")
    except Exception as e:
        st.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # è³ªå•å…¥åŠ›
    st.markdown("---")
    query = st.text_input(
        "ğŸ’¬ å¤§è°·é¸æ‰‹ã«è³ªå•ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: é‡çƒä»¥å¤–ã§èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        help="ã©ã‚“ãªè³ªå•ã§ã‚‚3å±¤ã‚·ã‚¹ãƒ†ãƒ ãŒé©åˆ‡ãªå›ç­”ã‚’è¦‹ã¤ã‘ã¾ã™"
    )
    
    if st.button("ğŸ” è³ªå•ã™ã‚‹", type="primary"):
        if query:
            with st.spinner("ğŸ¤– 3å±¤æ®µéšæ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
                result = rag_system.search(query)
            
            # çµæœè¡¨ç¤º
            st.markdown("---")
            
            # ä¿¡é ¼åº¦è¡¨ç¤º
            confidence_colors = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
            confidence_labels = {'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½'}
            
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                st.metric("ä½¿ç”¨ãƒ¬ã‚¤ãƒ¤ãƒ¼", f"Layer {result['layer']}")
            with col2:
                st.metric("ä¿¡é ¼åº¦", f"{confidence_colors[result['confidence']]} {confidence_labels[result['confidence']]}")
            with col3:
                st.info(result['message'])
            
            # å›ç­”è¡¨ç¤º
            st.markdown("### ğŸ’¬ å¤§è°·é¸æ‰‹ã®å›ç­”")
            st.markdown(f"**{result['response']}**")
            
            # ã‚½ãƒ¼ã‚¹è¡¨ç¤º
            st.markdown("### ğŸ“ å‚è€ƒæƒ…å ±")
            st.markdown(f"**å‡ºå…¸:** {result['source']}")
            
            # è©³ç´°çµæœï¼ˆLayer 2ã®å ´åˆï¼‰
            if result['layer'] == 2 and result['results']:
                with st.expander("ğŸ” æ¦‚å¿µæ¤œç´¢è©³ç´°çµæœ"):
                    for i, res in enumerate(result['results']):
                        st.markdown(f"**{i+1}. (ã‚¹ã‚³ã‚¢: {res['score']:.3f})**")
                        st.markdown(f"è³ªå•: {res['question']}")
                        st.markdown(f"å›ç­”: {res['answer'][:100]}...")
                        st.markdown("---")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    st.markdown("---")
    st.markdown("### ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    sample_questions = [
        "é‡çƒä»¥å¤–ã§èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "å®‡å®™æ—…è¡Œã«ã¤ã„ã¦ã©ã†æ€ã„ã¾ã™ã‹ï¼Ÿ",
        "AIã«ã¤ã„ã¦ã®è€ƒãˆã‚’èã‹ã›ã¦ãã ã•ã„",
        "å¥½ããªé£Ÿã¹ç‰©ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "å°†æ¥ã®å¤¢ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "å›°é›£ã‚’ä¹—ã‚Šè¶Šãˆã‚‹ç§˜è¨£ã¯ï¼Ÿ"
    ]
    
    cols = st.columns(2)
    for i, question in enumerate(sample_questions):
        with cols[i % 2]:
            if st.button(f"ğŸ“‹ {question}", key=f"sample_{i}"):
                st.session_state.sample_query = question
                st.rerun()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰è³ªå•ã‚’å–å¾—
    if 'sample_query' in st.session_state:
        query = st.session_state.sample_query
        del st.session_state.sample_query
        
        with st.spinner("ğŸ¤– 3å±¤æ®µéšæ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
            result = rag_system.search(query)
        
        # çµæœè¡¨ç¤ºï¼ˆä¸Šè¨˜ã¨åŒæ§˜ï¼‰
        st.markdown("---")
        st.markdown(f"**è³ªå•:** {query}")
        
        confidence_colors = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
        confidence_labels = {'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½'}
        
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            st.metric("ä½¿ç”¨ãƒ¬ã‚¤ãƒ¤ãƒ¼", f"Layer {result['layer']}")
        with col2:
            st.metric("ä¿¡é ¼åº¦", f"{confidence_colors[result['confidence']]} {confidence_labels[result['confidence']]}")
        with col3:
            st.info(result['message'])
        
        st.markdown("### ğŸ’¬ å¤§è°·é¸æ‰‹ã®å›ç­”")
        st.markdown(f"**{result['response']}**")
        
        st.markdown("### ğŸ“ å‚è€ƒæƒ…å ±")
        st.markdown(f"**å‡ºå…¸:** {result['source']}")

if __name__ == "__main__":
    main()