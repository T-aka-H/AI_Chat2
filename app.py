import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from typing import List, Tuple, Dict
import os

# è¿½åŠ : åŸ‹ã‚è¾¼ã¿ã¨æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
try:
    from sentence_transformers import SentenceTransformer
    _EMBED_AVAILABLE = True
except Exception:
    _EMBED_AVAILABLE = False

try:
    from janome.tokenizer import Tokenizer as JanomeTokenizer
    _JANOME_AVAILABLE = True
except Exception:
    _JANOME_AVAILABLE = False

# è¨­å®š
st.set_page_config(
    page_title="å¤§è°·ç¿”å¹³ AI Chat - 3å±¤æ®µéšRAGã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="âš¾",
    layout="wide"
)

# æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆJanomeãŒã‚ã‚Œã°åˆ©ç”¨ï¼‰
def tokenize_ja(text: str) -> List[str]:
    if not isinstance(text, str):
        return []
    if _JANOME_AVAILABLE:
        t = JanomeTokenizer()
        return [token.surface for token in t.tokenize(text)]
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“åˆ†å‰²
    return re.findall(r"\w+|[\u3040-\u30ff\u4e00-\u9fff]+", text)

# MMR (Maximal Marginal Relevance) å®Ÿè£…
def mmr(query_vec: np.ndarray, doc_vecs: np.ndarray, top_k: int = 5, lambda_mult: float = 0.5) -> List[int]:
    selected = []
    sim_to_query = cosine_similarity(query_vec.reshape(1, -1), doc_vecs)[0]
    candidates = list(range(len(doc_vecs)))
    if len(candidates) == 0:
        return []

    # 1ã¤ç›®ã¯æœ€ã‚‚è¿‘ã„ã‚‚ã®
    best_idx = int(np.argmax(sim_to_query))
    selected.append(best_idx)
    candidates.remove(best_idx)

    while len(selected) < min(top_k, len(doc_vecs)) and candidates:
        mmr_scores = []
        for c in candidates:
            redundancy = max([cosine_similarity(doc_vecs[c].reshape(1, -1), doc_vecs[s].reshape(1, -1))[0][0] for s in selected]) if selected else 0.0
            score = lambda_mult * sim_to_query[c] - (1 - lambda_mult) * redundancy
            mmr_scores.append((c, score))
        mmr_scores.sort(key=lambda x: x[1], reverse=True)
        best = mmr_scores[0][0]
        selected.append(best)
        candidates.remove(best)
    return selected

class OhtaniRAGSystem:
    def __init__(self, csv_path: str, use_embeddings: bool, embed_model_name: str):
        """å¤§è°·ç¿”å¹³RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        # CSVèª­ã¿è¾¼ã¿ï¼ˆRAGé…ä¸‹ãŒæœ€å°è¡Œãªã‚‰è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        df = pd.read_csv(csv_path)
        if len(df) < 50:
            parent_path = os.path.join(os.path.dirname(os.path.dirname(csv_path)), "ohtani_rag_final.csv")
            if os.path.exists(parent_path):
                df = pd.read_csv(parent_path)
        self.df = df

        # TF-IDFï¼ˆæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶å¯¾å¿œï¼‰
        analyzer = None
        if _JANOME_AVAILABLE:
            analyzer = tokenize_ja
        self.vectorizer = TfidfVectorizer(max_features=4000, tokenizer=analyzer)
        self.question_vectors = self.vectorizer.fit_transform(self.df['Question'])
        self.answer_vectors = TfidfVectorizer(max_features=4000, tokenizer=analyzer).fit_transform(self.df['Answer'])

        # åŸ‹ã‚è¾¼ã¿ï¼ˆä»»æ„ï¼‰
        self.use_embeddings = use_embeddings and _EMBED_AVAILABLE
        self.embed_model = None
        self.q_embed = None
        self.a_embed = None
        if self.use_embeddings:
            self.embed_model = SentenceTransformer(embed_model_name)
            self.q_embed = self.embed_model.encode(self.df['Question'].tolist(), normalize_embeddings=True)
            self.a_embed = self.embed_model.encode(self.df['Answer'].tolist(), normalize_embeddings=True)

        # å¤§è°·é¸æ‰‹ã®ç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        self.speech_patterns = self._extract_speech_patterns()
        # å¤§è°·é¸æ‰‹ã®ç­”ãˆæ–¹ã®ç™–ã‚’å­¦ç¿’
        self.style = self._learn_style()

    def _extract_speech_patterns(self) -> List[str]:
        """å¤§è°·é¸æ‰‹ã®å®Ÿéš›ã®ç™ºè¨€ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        patterns = []
        answers = self.df['Answer'].tolist()
        common_patterns = [
            r'ã¾ã ã¾ã .*?ã¨æ€ã„ã¾ã™',
            r'.*?ã®ãŠã‹ã’ã§.*?',
            r'.*?ã‹ã‚‰å­¦ã¶ã“ã¨ãŒ.*?',
            r'ãã†ã§ã™ã­.*?',
            r'.*?ã ã¨æ€ã†ã®ã§.*?',
            r'.*?ã¨ã„ã†ã®ã¯.*?',
            r'.*?ã‹ãªã¨æ€ã„ã¾ã™',
            r'.*?ã˜ã‚ƒãªã„ã‹ãªã¨.*?',
            r'.*?ã—ã¦ã„ããŸã„ãªã¨æ€ã„ã¾ã™',
            r'.*?ã¨ã„ã†ã“ã¨ã§ã™',
        ]
        for answer in answers:
            for pattern in common_patterns:
                matches = re.findall(pattern, answer)
                patterns.extend(matches[:3])
        return list(set(patterns))[:50]

    def _learn_style(self) -> Dict[str, List[str]]:
        """Answeråˆ—ã‹ã‚‰ã€ç­”ãˆæ–¹ã®ç™–ã€ã‚’å­¦ç¿’ã™ã‚‹ç°¡æ˜“ã‚¹ã‚¿ã‚¤ãƒ«ãƒ¢ãƒ‡ãƒ«"""
        answers = [str(a) for a in self.df['Answer'].tolist() if isinstance(a, str)]
        starters_count = {}
        endings_count = {}
        hesitations = ["ãã†ã§ã™ã­", "ã‚“ãƒ¼", "ã¾ã‚", "ã‚ã®", "ã‚„ã£ã±ã‚Š"]
        connectors = ["ã¾ãš", "ãã®ä¸Šã§", "ãªã®ã§", "ãŸã ", "ä¸€æ–¹ã§", "ã—ã£ã‹ã‚Š", "å¾ã€…ã«"]
        for a in answers:
            # æ–‡ã”ã¨
            sents = [s for s in re.split(r"[ã€‚!?ï¼ï¼Ÿã€]", a) if s.strip()]
            if sents:
                start = sents[0].strip()[:6]
                starters_count[start] = starters_count.get(start, 0) + 1
            # ä»£è¡¨çš„ãªæ–‡æœ«
            m = re.findall(r"(ã¨æ€ã„ã¾ã™|ã‹ãªã¨æ€ã„ã¾ã™|ã ã¨æ€ã„ã¾ã™|ã˜ã‚ƒãªã„ã‹ãªã¨|ã§ã¯ãªã„ã‹ãª|ã—ã¦ã„ããŸã„|ã§ã™|ã¾ã™)\s*$", a)
            for e in m:
                endings_count[e] = endings_count.get(e, 0) + 1
        # ä¸Šä½æŠ½å‡º
        def topk(d: Dict[str, int], k: int) -> List[str]:
            return [w for w, _ in sorted(d.items(), key=lambda x: x[1], reverse=True)[:k]]
        style = {
            "starters": topk(starters_count, 10) or ["ãã†ã§ã™ã­"],
            "endings": topk(endings_count, 10) or ["ã¨æ€ã„ã¾ã™"],
            "hesitations": hesitations,
            "connectors": connectors,
        }
        return style

    def _apply_style(self, query: str, core_text: str | None = None) -> str:
        """å­¦ç¿’ã—ãŸç™–ã‚’ä½¿ã£ã¦å¿œç­”ã‚’åŒ…ã‚€"""
        import random
        starter = random.choice(self.style["starters"]) if self.style["starters"] else "ãã†ã§ã™ã­"
        hesi = random.choice(self.style["hesitations"]) if np.random.rand() < 0.6 else ""
        conn = random.choice(self.style["connectors"]) if np.random.rand() < 0.5 else ""
        ending = random.choice(self.style["endings"]) if self.style["endings"] else "ã¨æ€ã„ã¾ã™"
        prefix = f"{starter}ã€" if starter else ""
        if hesi and not prefix.startswith(hesi):
            prefix = hesi + "ã€" + prefix
        body = core_text.strip() if core_text else f"{query}ã«ã¤ã„ã¦ã¯{ending}ã€‚"
        if not body.endswith("ã€‚"):
            body += "ã€‚"
        tail = f"{conn}ã€ã“ã‚Œã‹ã‚‰ã‚‚ã—ã£ã‹ã‚Šæº–å‚™ã—ã¦ã„ããŸã„{ending}ã€‚" if conn else f"ã“ã‚Œã‹ã‚‰ã‚‚ç¶™ç¶šã—ã¦ã„ããŸã„{ending}ã€‚"
        return prefix + body + tail

    def _similarities(self, query: str, use_answer_space: bool = False) -> np.ndarray:
        """åŸ‹ã‚è¾¼ã¿ or TF-IDF ã§é¡ä¼¼åº¦ã‚’è¿”ã™"""
        if self.use_embeddings:
            vec = self.embed_model.encode([query], normalize_embeddings=True)[0]
            mat = self.a_embed if use_answer_space else self.q_embed
            return cosine_similarity(vec.reshape(1, -1), mat)[0]
        else:
            vectorizer = (TfidfVectorizer(max_features=4000, tokenizer=tokenize_ja)
                         if _JANOME_AVAILABLE else self.vectorizer)
            vec = vectorizer.fit(self.df['Question']).transform([query])
            mat = self.answer_vectors if use_answer_space else self.question_vectors
            return cosine_similarity(vec, mat)[0]

    def layer1_direct_search(self, query: str, threshold: float = 0.7, top_k: int = 1, mmr_lambda: float = 0.5) -> Tuple[List[Dict], str]:
        """Layer 1: ç›´æ¥æ¤œç´¢ â†’ ä¿¡é ¼åº¦: é«˜"""
        sims = self._similarities(query, use_answer_space=False)
        if top_k == 1:
            best_idx = int(np.argmax(sims))
            best_score = float(sims[best_idx])
            if best_score >= threshold:
                return [{
                    'question': self.df.iloc[best_idx]['Question'],
                    'answer': self.df.iloc[best_idx]['Answer'],
                    'score': best_score,
                    'id': int(self.df.iloc[best_idx]['ID']),
                    'confidence': 'high'
                }], "ç›´æ¥æ¤œç´¢ã§é«˜ç²¾åº¦ãƒãƒƒãƒã‚’ç™ºè¦‹"
            return [], "ç›´æ¥æ¤œç´¢ã§ã¯è©²å½“ãªã—"
        # MMRã§è¤‡æ•°å–å¾—
        idxs = mmr(sims.reshape(-1, 1), sims.reshape(-1, 1), top_k=top_k, lambda_mult=mmr_lambda)
        results = []
        for i in idxs:
            if sims[i] >= threshold:
                results.append({
                    'question': self.df.iloc[i]['Question'],
                    'answer': self.df.iloc[i]['Answer'],
                    'score': float(sims[i]),
                    'id': int(self.df.iloc[i]['ID']),
                    'confidence': 'high'
                })
        return (results, f"ç›´æ¥æ¤œç´¢ã§{len(results)}ä»¶") if results else ([], "ç›´æ¥æ¤œç´¢ã§ã¯è©²å½“ãªã—")

    def layer2_concept_search(self, query: str, threshold: float = 0.5, top_k: int = 3, mmr_lambda: float = 0.5) -> Tuple[List[Dict], str]:
        """Layer 2: æ¦‚å¿µæ¤œç´¢ â†’ ä¿¡é ¼åº¦: ä¸­"""
        concept_expansions = {
            'AI': ['æŠ€è¡“', 'æ–°ã—ã„', 'å­¦ç¿’', 'é€²æ­©', 'æŒ‘æˆ¦', 'æœªæ¥'],
            'å®‡å®™': ['å¤¢', 'æŒ‘æˆ¦', 'æ–°ã—ã„', 'ç›®æ¨™', 'åºƒã„'],
            'æ–™ç†': ['é£Ÿäº‹', 'å¥½ã', 'æ¥½ã—ã„', 'ãŠã„ã—ã„'],
            'æ˜ ç”»': ['è¦‹ã‚‹', 'æ¥½ã—ã„', 'æ™‚é–“', 'ãƒªãƒ©ãƒƒã‚¯ã‚¹'],
            'éŸ³æ¥½': ['èã', 'å¥½ã', 'ãƒªãƒ©ãƒƒã‚¯ã‚¹', 'æ¥½ã—ã„'],
            'å®¶æ—': ['å¤§åˆ‡', 'æ”¯ãˆ', 'ã‚ã‚ŠãŒãŸã„', 'æ„Ÿè¬'],
            'å‹é”': ['ä»²é–“', 'ãƒãƒ¼ãƒ ãƒ¡ãƒ¼ãƒˆ', 'å¤§åˆ‡', 'ä¿¡é ¼'],
            'å°†æ¥': ['ç›®æ¨™', 'å¤¢', 'æŒ‘æˆ¦', 'é ‘å¼µã‚‹'],
            'å›°é›£': ['æŒ‘æˆ¦', 'ä¹—ã‚Šè¶Šãˆã‚‹', 'å­¦ã¶', 'æˆé•·'],
            'æˆåŠŸ': ['åŠªåŠ›', 'ç¶™ç¶š', 'ãƒãƒ¼ãƒ ', 'æ„Ÿè¬'],
        }
        expanded_query = query
        for concept, keywords in concept_expansions.items():
            if concept in query:
                expanded_query += ' ' + ' '.join(keywords)
        sims = self._similarities(expanded_query, use_answer_space=False)
        idxs = np.argsort(sims)[-top_k:][::-1]
        results = []
        for idx in idxs:
            if sims[idx] >= threshold:
                results.append({
                    'question': self.df.iloc[idx]['Question'],
                    'answer': self.df.iloc[idx]['Answer'],
                    'score': float(sims[idx]),
                    'id': int(self.df.iloc[idx]['ID']),
                    'confidence': 'medium'
                })
        return (results, f"æ¦‚å¿µæ¤œç´¢ã§{len(results)}ä»¶ç™ºè¦‹ (æ‹¡å¼µ: {expanded_query})") if results else ([], "æ¦‚å¿µæ¤œç´¢ã§ã‚‚è©²å½“ãªã—")

    def layer3_pattern_generation(self, query: str) -> Tuple[str, str]:
        """Layer 3: ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ â†’ ä¿¡é ¼åº¦: ä½"""
        import random
        selected_patterns = random.sample(self.speech_patterns, min(3, len(self.speech_patterns)))
        # å›ç­”ç©ºé–“ã«å¯¾ã™ã‚‹è¿‘å‚ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ‹å€Ÿ
        sims = self._similarities(query, use_answer_space=True)
        top_idx = int(np.argmax(sims))
        related_answer = self.df.iloc[top_idx]['Answer']
        generated_response = self._generate_pattern_response(query, selected_patterns, related_answer)
        return generated_response, f"ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ (ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(selected_patterns)}å€‹)"

    def _generate_pattern_response(self, query: str, patterns: List[str], related_answer: str) -> str:
        # æ—¢å­˜ã®çµ„ã¿ç«‹ã¦ã«å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨
        base_responses = [
            f"{query}ã«é–¢ã—ã¦ã¯",
            f"ã†ãƒ¼ã‚“ã€{query}ã¨ã„ã†ã®ã¯",
            f"{query}ã«ã¤ã„ã¦ã¯",
        ]
        middle_parts = [
            "æ–°ã—ã„ã“ã¨ã«æŒ‘æˆ¦ã™ã‚‹ã®ã¯ç´ æ™´ã‚‰ã—ã„ã“ã¨ã ã¨æ€ã„ã¾ã™",
            "ã¾ã ã¾ã å­¦ã¶ã“ã¨ãŒãŸãã•ã‚“ã‚ã‚‹ã¨æ„Ÿã˜ã¦ã„ã¾ã™",
            "ã“ã‚Œã‹ã‚‰ã‚‚åŠªåŠ›ã‚’ç¶šã‘ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™",
            "ãƒãƒ¼ãƒ ã®ã¿ã‚“ãªã®ãŠã‹ã’ã§æˆé•·ã§ãã¦ã„ã¾ã™",
        ]
        ending_parts = [
            "é‡çƒä»¥å¤–ã®ã“ã¨ã‹ã‚‰ã‚‚å­¦ã¶ã“ã¨ãŒãŸãã•ã‚“ã‚ã‚‹ã¨æ€ã£ã¦ã„ã¾ã™ã€‚",
            "ã“ã‚Œã‹ã‚‰ã‚‚é ‘å¼µã£ã¦ã„ããŸã„ãªã¨æ€ã„ã¾ã™ã€‚",
            "ç¶™ç¶šã—ã¦ã„ãã“ã¨ãŒå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚",
        ]
        import random
        core = random.choice(base_responses) + random.choice(middle_parts) + "ã€‚" + random.choice(ending_parts)
        return self._apply_style(query, core)

    def aggregate_answers(self, hits: List[Dict], max_chars: int = 400, style: bool = False, query: str = "") -> str:
        """è¤‡æ•°æ ¹æ‹ ã‚’çŸ­ãçµ±åˆï¼ˆä»»æ„ã§ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨ï¼‰"""
        if not hits:
            return ""
        texts = []
        for h in hits:
            ans = str(h['answer']).strip()
            if ans:
                texts.append(ans)
        core = " ".join(texts)[:max_chars]
        return self._apply_style(query, core) if style else core

    def search(self, query: str, l1_th: float, l2_th: float, top_k: int, mmr_lambda: float, style_on_aggregate: bool) -> Dict:
        """3å±¤æ®µéšæ¤œç´¢ã®å®Ÿè¡Œï¼ˆè¨­å®šåæ˜ ï¼‰"""
        # Layer 1
        results1, msg1 = self.layer1_direct_search(query, threshold=l1_th, top_k=top_k, mmr_lambda=mmr_lambda)
        if results1:
            answer = results1[0]['answer'] if top_k == 1 else self.aggregate_answers(results1, style=style_on_aggregate, query=query)
            return {
                'layer': 1,
                'results': results1,
                'message': msg1,
                'confidence': 'high',
                'response': answer,
                'source': f"; ".join([f"ID {r['id']}: {r['question'][:40]}" for r in results1])
            }

        # Layer 2
        results2, msg2 = self.layer2_concept_search(query, threshold=l2_th, top_k=top_k, mmr_lambda=mmr_lambda)
        if results2:
            answer = results2[0]['answer'] if top_k == 1 else self.aggregate_answers(results2, style=style_on_aggregate, query=query)
            return {
                'layer': 2,
                'results': results2,
                'message': msg2,
                'confidence': 'medium',
                'response': answer,
                'source': f"; ".join([f"ID {r['id']}: {r['question'][:40]}" for r in results2])
            }

        # Layer 3
        generated_response, msg3 = self.layer3_pattern_generation(query)
        return {
            'layer': 3,
            'results': [],
            'message': msg3,
            'confidence': 'low',
            'response': generated_response,
            'source': "å¤§è°·é¸æ‰‹ã®ç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ç”Ÿæˆ"
        }


def main():
    st.title("âš¾ å¤§è°·ç¿”å¹³ AI Chat")
    st.subheader("ğŸš€ 3å±¤æ®µéšRAGã‚·ã‚¹ãƒ†ãƒ  - çœŸã®RAGä¾¡å€¤ã‚’ç™ºæ®ã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")
    use_embeddings = st.sidebar.checkbox("Sentence-Transformers åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ã†", value=_EMBED_AVAILABLE)
    embed_model = st.sidebar.text_input("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«", value="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    top_k = st.sidebar.slider("Top K (æ ¹æ‹ ä»¶æ•°)", 1, 5, 1)
    l1_th = st.sidebar.slider("Layer1 é–¾å€¤", 0.0, 1.0, 0.7, 0.05)
    l2_th = st.sidebar.slider("Layer2 é–¾å€¤", 0.0, 1.0, 0.5, 0.05)
    mmr_lambda = st.sidebar.slider("MMR Î» (å¤šæ§˜æ€§)", 0.0, 1.0, 0.5, 0.05)
    style_on_aggregate = st.sidebar.checkbox("è¤‡æ•°æ ¹æ‹ ã‚’å¤§è°·ã‚¹ã‚¿ã‚¤ãƒ«ã§è¦ç´„ã™ã‚‹", value=True)

    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    @st.cache_resource
    def load_rag_system_cached(_use_embeddings: bool, _model: str):
        return OhtaniRAGSystem('ohtani_rag_final.csv', _use_embeddings, _model)
    try:
        rag_system = load_rag_system_cached(use_embeddings, embed_model)
        st.success(f"âœ… RAGåˆæœŸåŒ–å®Œäº† ({len(rag_system.df)}ä»¶ / embeddings={rag_system.use_embeddings})")
    except Exception as e:
        st.error(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    st.markdown("---")
    query = st.text_input(
        "ğŸ’¬ å¤§è°·é¸æ‰‹ã«è³ªå•ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: é‡çƒä»¥å¤–ã§èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        help="ã©ã‚“ãªè³ªå•ã§ã‚‚3å±¤ã‚·ã‚¹ãƒ†ãƒ ãŒé©åˆ‡ãªå›ç­”ã‚’è¦‹ã¤ã‘ã¾ã™"
    )

    if st.button("ğŸ” è³ªå•ã™ã‚‹", type="primary"):
        if query.strip():
            with st.spinner("ğŸ¤– 3å±¤æ®µéšæ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
                result = rag_system.search(query, l1_th=l1_th, l2_th=l2_th, top_k=top_k, mmr_lambda=mmr_lambda, style_on_aggregate=style_on_aggregate)

            st.markdown("---")
            confidence_colors = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
            confidence_labels = {'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½'}
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                st.metric("ä½¿ç”¨ãƒ¬ã‚¤ãƒ¤ãƒ¼", f"Layer {result['layer']}")
            with col2:
                st.metric("ä¿¡é ¼åº¦", f"{confidence_colors[result['confidence']]} {confidence_labels[result['confidence']]}")
            with col3:
                st.info(result['message'])

            st.markdown("### ğŸ’¬ å¤§è°·é¸æ‰‹ã®å›ç­”")
            st.markdown(f"**{result['response']}**")

            st.markdown("### ğŸ“ å‚è€ƒæƒ…å ±")
            st.markdown(f"**å‡ºå…¸:** {result['source']}")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    sample_questions = [
        "é‡çƒä»¥å¤–ã§èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "å®‡å®™æ—…è¡Œã«ã¤ã„ã¦ã©ã†æ€ã„ã¾ã™ã‹ï¼Ÿ",
        "AIã«ã¤ã„ã¦ã®è€ƒãˆã‚’èã‹ã›ã¦ãã ã•ã„",
        "å¥½ããªé£Ÿã¹ç‰©ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "å°†æ¥ã®å¤¢ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "å›°é›£ã‚’ä¹—ã‚Šè¶Šãˆã‚‹ç§˜è¨£ã¯ï¼Ÿ"
    ]
    cols = st.columns(2)
    for i, q in enumerate(sample_questions):
        with cols[i % 2]:
            if st.button(f"ğŸ“‹ {q}", key=f"sample_{i}"):
                st.session_state.sample_query = q
                st.rerun()

    if 'sample_query' in st.session_state:
        query = st.session_state.sample_query
        del st.session_state.sample_query
        with st.spinner("ğŸ¤– 3å±¤æ®µéšæ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
            result = rag_system.search(query, l1_th=l1_th, l2_th=l2_th, top_k=top_k, mmr_lambda=mmr_lambda, style_on_aggregate=style_on_aggregate)
        st.markdown("---")
        confidence_colors = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
        confidence_labels = {'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½'}
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            st.metric("ä½¿ç”¨ãƒ¬ã‚¤ãƒ¤ãƒ¼", f"Layer {result['layer']}")
        with col2:
            st.metric("ä¿¡é ¼åº¦", f"{confidence_colors[result['confidence']]} {confidence_labels[result['confidence']]}")
        with col3:
            st.info(result['message'])
        st.markdown("### ğŸ’¬ å¤§è°·é¸æ‰‹ã®å›ç­”")
        st.markdown(f"**{result['response']}**")
        st.markdown("### ğŸ“ å‚è€ƒæƒ…å ±")
        st.markdown(f"**å‡ºå…¸:** {result['source']}")

if __name__ == "__main__":
    main()