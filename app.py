import streamlit as st
import pandas as pd
import numpy as np
import json
import re
import random
import os
from typing import Dict, List, Optional, Tuple
from collections import Counter
import math
import time
import requests

# è¨­å®š
st.set_page_config(
    page_title="AIå¤§è°· - é«˜é€Ÿç‰ˆ",
    layout="wide"
)

# è»½é‡ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚¯ãƒ©ã‚¹ï¼ˆscikit-learnä¸è¦ï¼‰
class LightweightTextSearch:
    """è»½é‡TF-IDFæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, texts: List[str], max_features: int = 2000):
        self.texts = texts
        self.max_features = max_features
        self.vocab = self._build_vocabulary()
        self.idf_vector = self._compute_idf()
    
    def _tokenize(self, text: str) -> List[str]:
        """æ—¥æœ¬èªå¯¾å¿œãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º"""
        if not isinstance(text, str):
            return []
        tokens = re.findall(r'[\w\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]+', text.lower())
        return [token for token in tokens if len(token) > 1]
    
    def _build_vocabulary(self) -> Dict[str, int]:
        """èªå½™è¾æ›¸æ§‹ç¯‰"""
        all_tokens = []
        for text in self.texts:
            all_tokens.extend(self._tokenize(text))
        
        token_counts = Counter(all_tokens)
        top_tokens = [token for token, count in token_counts.most_common(self.max_features)]
        
        return {token: idx for idx, token in enumerate(top_tokens)}
    
    def _compute_tf(self, tokens: List[str]) -> np.ndarray:
        """TFè¨ˆç®—"""
        tf_vector = np.zeros(len(self.vocab))
        if not tokens:
            return tf_vector
            
        token_counts = Counter(tokens)
        total_tokens = len(tokens)
        
        for token, count in token_counts.items():
            if token in self.vocab:
                tf_vector[self.vocab[token]] = count / total_tokens
        
        return tf_vector
    
    def _compute_idf(self) -> np.ndarray:
        """IDFè¨ˆç®—"""
        idf_vector = np.zeros(len(self.vocab))
        num_docs = len(self.texts)
        
        for token, token_idx in self.vocab.items():
            doc_count = sum(1 for text in self.texts if token in self._tokenize(text))
            if doc_count > 0:
                idf_vector[token_idx] = math.log(num_docs / doc_count)
        
        return idf_vector
    
    def _text_to_tfidf(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        tokens = self._tokenize(text)
        tf_vector = self._compute_tf(tokens)
        return tf_vector * self.idf_vector
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """é¡ä¼¼ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢"""
        query_tfidf = self._text_to_tfidf(query)
        similarities = []
        
        for i, text in enumerate(self.texts):
            text_tfidf = self._text_to_tfidf(text)
            similarity = self.cosine_similarity(query_tfidf, text_tfidf)
            similarities.append((i, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

# è¶…è»½é‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
class KeywordSearch:
    """è»½é‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢"""
    
    def __init__(self, texts: List[str]):
        self.texts = texts
        self.keyword_index = self._build_index()
    
    def _tokenize(self, text: str) -> List[str]:
        if not isinstance(text, str):
            return []
        return re.findall(r'[\w\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]+', text.lower())
    
    def _build_index(self) -> Dict[str, List[int]]:
        index = {}
        for doc_id, text in enumerate(self.texts):
            tokens = self._tokenize(text)
            for token in set(tokens):
                if len(token) > 1:
                    if token not in index:
                        index[token] = []
                    index[token].append(doc_id)
        return index
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        query_tokens = set(self._tokenize(query))
        doc_scores = {}
        
        for token in query_tokens:
            if token in self.keyword_index:
                for doc_id in self.keyword_index[token]:
                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1
        
        if not doc_scores:
            return []
        
        max_score = max(doc_scores.values())
        results = [(doc_id, score/max_score) for doc_id, score in doc_scores.items()]
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:top_k]

# ãƒ¡ã‚¤ãƒ³RAGã‚·ã‚¹ãƒ†ãƒ 
class FastOhtaniRAG:
    """é«˜é€Ÿå¤§è°·ç¿”å¹³RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, csv_path: str):
        self.df = self._load_data(csv_path)
        self.questions = self.df['Question'].fillna('').astype(str).tolist()
        self.answers = self.df['Answer'].fillna('').astype(str).tolist()
        
        # æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.tfidf_search = LightweightTextSearch(self.questions)
        self.keyword_search = KeywordSearch(self.questions)
        self.answer_search = KeywordSearch(self.answers)
        
        # å¤§è°·é¸æ‰‹ã®è©±ã—æ–¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.ohtani_patterns = self._extract_speech_patterns()
    
    def _load_data(self, csv_path: str) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            df = pd.read_csv(csv_path)
            if len(df) < 50:
                parent_path = os.path.join(os.path.dirname(os.path.dirname(csv_path)), "ohtani_rag_final.csv")
                if os.path.exists(parent_path):
                    df = pd.read_csv(parent_path)
            return df
        except FileNotFoundError:
            return self._create_sample_data()
    
    def _create_sample_data(self) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
        return pd.DataFrame({
            'ID': range(1, 21),
            'Question': [
                'é‡çƒä»¥å¤–ã§èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ',
                'ã‚ªãƒ•ã‚·ãƒ¼ã‚ºãƒ³ã¯ã©ã†éã”ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ',
                'å¥½ããªé£Ÿã¹ç‰©ã¯ä½•ã§ã™ã‹ï¼Ÿ',
                'å°†æ¥ã®ç›®æ¨™ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„',
                'ãƒãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆã¨ã®é–¢ä¿‚ã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ',
                'å›°é›£ãªæ™‚æœŸã‚’ã©ã†ä¹—ã‚Šè¶Šãˆã¾ã™ã‹ï¼Ÿ',
                'æ—¥æœ¬ã¨ã‚¢ãƒ¡ãƒªã‚«ã®é•ã„ã¯ï¼Ÿ',
                'ãƒ•ã‚¡ãƒ³ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãŠé¡˜ã„ã—ã¾ã™',
                'ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§å¤§åˆ‡ã«ã—ã¦ã„ã‚‹ã“ã¨ã¯ï¼Ÿ',
                'é‡çƒã‚’å§‹ã‚ãŸãã£ã‹ã‘ã¯ï¼Ÿ',
                'ãƒªãƒ©ãƒƒã‚¯ã‚¹æ–¹æ³•ã¯ï¼Ÿ',
                'å°Šæ•¬ã™ã‚‹é¸æ‰‹ã¯ã„ã¾ã™ã‹ï¼Ÿ',
                'å­ä¾›ãŸã¡ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯ï¼Ÿ',
                'ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã‚’æ„Ÿã˜ã‚‹ã“ã¨ã¯ï¼Ÿ',
                'ä»Šã‚·ãƒ¼ã‚ºãƒ³ã®ç›®æ¨™ã¯ï¼Ÿ',
                'ã‚³ãƒ¼ãƒã¨ã®é–¢ä¿‚ã«ã¤ã„ã¦',
                'ã‘ãŒã‚’ã—ãŸæ™‚ã®æ°—æŒã¡ã¯ï¼Ÿ',
                'ã‚ªãƒ¼ãƒ«ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã®æ„Ÿæƒ³ã¯ï¼Ÿ',
                'é‡çƒã®é­…åŠ›ã¨ã¯ï¼Ÿ',
                'ã“ã‚Œã‹ã‚‰ã®é‡çƒç•Œã«ã¤ã„ã¦'
            ],
            'Answer': [
                'ãã†ã§ã™ã­ã€æ–™ç†ã‚’ã™ã‚‹ã®ãŒå¥½ãã§ã™ã­ã€‚æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã«æŒ‘æˆ¦ã™ã‚‹ã“ã¨ã§ã€é‡çƒä»¥å¤–ã§ã‚‚æˆé•·ã§ãã‚‹ã¨æ€ã£ã¦ã„ã¾ã™ã€‚',
                'ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚‚ã¡ã‚ã‚“ã§ã™ãŒã€ãƒªãƒ©ãƒƒã‚¯ã‚¹ã™ã‚‹ã“ã¨ã‚‚å¤§åˆ‡ã«ã—ã¦ã„ã¾ã™ã€‚èª­æ›¸ã‚’ã—ãŸã‚Šã€æ˜ ç”»ã‚’è¦‹ãŸã‚Šã—ã¦ã„ã¾ã™ã€‚',
                'å’Œé£ŸãŒä¸€ç•ªå¥½ãã§ã™ã­ã€‚ç‰¹ã«æ¯ãŒä½œã£ã¦ãã‚ŒãŸæ–™ç†ã®å‘³ã¯å¿˜ã‚Œã‚‰ã‚Œã¾ã›ã‚“ã€‚',
                'å¸¸ã«æˆé•·ã—ç¶šã‘ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚é‡çƒã‚’é€šã˜ã¦å¤šãã®äººã«å½±éŸ¿ã‚’ä¸ãˆã‚‰ã‚Œã‚‹é¸æ‰‹ã«ãªã‚ŠãŸã„ã§ã™ã€‚',
                'ãƒãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆã¯ã¿ã‚“ãªç´ æ™´ã‚‰ã—ã„äººãŸã¡ã§ã™ã€‚ãŠäº’ã„ã‚’é«˜ã‚åˆãˆã‚‹é–¢ä¿‚ã‚’ç¯‰ã‘ã¦ã„ã‚‹ã¨æ€ã„ã¾ã™ã€‚',
                'å›°é›£ãªæ™‚ã“ãã€åŸºæœ¬ã«ç«‹ã¡æˆ»ã‚‹ã“ã¨ã‚’å¤§åˆ‡ã«ã—ã¦ã„ã¾ã™ã€‚ãã—ã¦ã€æ”¯ãˆã¦ãã‚Œã‚‹äººãŸã¡ã¸ã®æ„Ÿè¬ã‚’å¿˜ã‚Œãšã«ã€‚',
                'æ–‡åŒ–ã®é•ã„ã¯ã‚ã‚Šã¾ã™ãŒã€é‡çƒã¸ã®æƒ…ç†±ã¯åŒã˜ã§ã™ã€‚ã©ã¡ã‚‰ã®å›½ã‹ã‚‰ã‚‚å­¦ã¶ã“ã¨ãŒãŸãã•ã‚“ã‚ã‚Šã¾ã™ã€‚',
                'ã„ã¤ã‚‚å¿œæ´ã—ã¦ãã ã•ã£ã¦ã€æœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚çš†ã•ã‚“ã®å£°æ´ãŒåŠ›ã«ãªã£ã¦ã„ã¾ã™ã€‚',
                'ç¶™ç¶šã™ã‚‹ã“ã¨ãŒä¸€ç•ªå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚å°ã•ãªã“ã¨ã®ç©ã¿é‡ã­ãŒå¤§ããªæˆæœã«ã¤ãªãŒã‚Šã¾ã™ã€‚',
                'çˆ¶ã®å½±éŸ¿ãŒå¤§ãã‹ã£ãŸã§ã™ã€‚é‡çƒã®æ¥½ã—ã•ã‚’æ•™ãˆã¦ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚',
                'è‡ªç„¶ã®ä¸­ã§éã”ã™ã“ã¨ãŒå¤šã„ã§ã™ã­ã€‚æ•£æ­©ã‚’ã—ãŸã‚Šã€ç©ºã‚’çœºã‚ãŸã‚Šã—ã¦ã„ã¾ã™ã€‚',
                'ã‚¤ãƒãƒ­ãƒ¼é¸æ‰‹ã«ã¯æœ¬å½“ã«å¤šãã®ã“ã¨ã‚’å­¦ã°ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚',
                'å¥½ããªã“ã¨ã‚’è¦‹ã¤ã‘ã¦ã€ãã‚Œã‚’å¤§åˆ‡ã«ã—ã¦ã»ã—ã„ã§ã™ã€‚ãã—ã¦è«¦ã‚ãšã«ç¶šã‘ã¦ãã ã•ã„ã€‚',
                'ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã¯æ„Ÿã˜ã¾ã™ãŒã€ãã‚Œã‚’æ¥½ã—ã‚ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚',
                'ãƒãƒ¼ãƒ ä¸€ä¸¸ã¨ãªã£ã¦ã€è‰¯ã„çµæœã‚’æ®‹ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚',
                'ã‚³ãƒ¼ãƒã‹ã‚‰ã¯ãŸãã•ã‚“ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã‚‚ã‚‰ã£ã¦ã„ã¾ã™ã€‚ã¨ã¦ã‚‚æ„Ÿè¬ã—ã¦ã„ã¾ã™ã€‚',
                'ã‘ãŒã¯è¾›ã„ã§ã™ãŒã€ãã‚Œã‚‚çµŒé¨“ã®ä¸€ã¤ã ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚',
                'ãƒ•ã‚¡ãƒ³ã®çš†ã•ã‚“ã¨ä¸€ç·’ã«æ¥½ã—ã„æ™‚é–“ã‚’éã”ã›ã¾ã—ãŸã€‚',
                'é‡çƒã¯äººã¨äººã‚’ã¤ãªã’ã‚‹ç´ æ™´ã‚‰ã—ã„ã‚¹ãƒãƒ¼ãƒ„ã ã¨æ€ã„ã¾ã™ã€‚',
                'è‹¥ã„é¸æ‰‹ãŸã¡ã®æˆé•·ãŒæ¥½ã—ã¿ã§ã™ã€‚é‡çƒç•Œå…¨ä½“ãŒã‚ˆã‚Šè‰¯ããªã‚‹ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚'
            ]
        })
    
    def _extract_speech_patterns(self) -> Dict:
        """å¤§è°·é¸æ‰‹ã®è©±ã—æ–¹ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        return {
            'starters': ['ãã†ã§ã™ã­', 'ã†ãƒ¼ã‚“', 'ã‚„ã£ã±ã‚Š', 'ã¾ã‚'],
            'endings': ['ã¨æ€ã„ã¾ã™', 'ã‹ãªã¨æ€ã„ã¾ã™', 'ã˜ã‚ƒãªã„ã‹ãªã¨', 'ã§ã™ã­'],
            'values': ['æ„Ÿè¬', 'ãƒãƒ¼ãƒ ', 'æˆé•·', 'æŒ‘æˆ¦', 'ç¶™ç¶š', 'åŠªåŠ›'],
            'humble': ['ã¾ã ã¾ã ', 'å‹‰å¼·ã«ãªã‚Šã¾ã™', 'ã‚ã‚ŠãŒãŸã„', 'ãŠã‹ã’ã§']
        }
    
    def search(self, query: str, method: str = 'hybrid', threshold: float = 0.3) -> Dict:
        """RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  - Retrieval-Augmented Generation"""
        
        search_results = []
        
        # Layer 1: TF-IDFæ¤œç´¢ï¼ˆè³ªå•ç©ºé–“ï¼‰
        if method in ['tfidf', 'hybrid']:
            tfidf_results = self.tfidf_search.search(query, top_k=3)
            if tfidf_results and tfidf_results[0][1] >= threshold:
                idx, score = tfidf_results[0]
                search_results = tfidf_results
                return {
                    'layer': 1,
                    'method': 'TF-IDF RAG',
                    'confidence': 'high' if score > 0.5 else 'medium',
                    'response': self.answers[idx],
                    'source': f"RAGæ¤œç´¢ - ID {self.df.iloc[idx]['ID']}: {self.questions[idx][:50]}...",
                    'score': float(score),
                    'search_results': search_results,
                    'retrieved_docs': self._format_retrieved_docs(tfidf_results)
                }
        
        # Layer 2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆè³ªå•ç©ºé–“ï¼‰
        if method in ['keyword', 'hybrid']:
            keyword_results = self.keyword_search.search(query, top_k=3)
            if keyword_results and keyword_results[0][1] >= threshold * 0.7:
                idx, score = keyword_results[0]
                search_results = keyword_results
                return {
                    'layer': 2,
                    'method': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰RAG',
                    'confidence': 'medium',
                    'response': self.answers[idx],
                    'source': f"RAGæ¤œç´¢ - ID {self.df.iloc[idx]['ID']}: {self.questions[idx][:50]}...",
                    'score': float(score),
                    'search_results': search_results,
                    'retrieved_docs': self._format_retrieved_docs(keyword_results)
                }
        
        # Layer 3: å›ç­”ç©ºé–“æ¤œç´¢
        answer_results = self.answer_search.search(query, top_k=3)
        if answer_results and answer_results[0][1] >= threshold * 0.5:
            idx, score = answer_results[0]
            search_results = answer_results
            return {
                'layer': 3,
                'method': 'å›ç­”ç©ºé–“RAG',
                'confidence': 'medium',
                'response': self.answers[idx],
                'source': f"RAGæ¤œç´¢ - ID {self.df.iloc[idx]['ID']}: å›ç­”ã‹ã‚‰æ¤œç´¢",
                'score': float(score),
                'search_results': search_results,
                'retrieved_docs': self._format_retrieved_docs(answer_results, answer_space=True)
            }
        
        # Layer 4: è¤‡æ•°æ–‡æ›¸ã‚’çµ±åˆã—ã¦RAGç”Ÿæˆ
        all_results = self.keyword_search.search(query, top_k=5)
        if all_results:
            search_results = all_results
            # è¤‡æ•°ã®é–¢é€£æ–‡æ›¸ã‚’å–å¾—ã—ã¦çµ±åˆ
            aggregated_context = self._aggregate_multiple_docs(all_results[:3])
            return {
                'layer': 4,
                'method': 'è¤‡æ•°æ–‡æ›¸RAG',
                'confidence': 'medium',
                'response': aggregated_context,
                'source': f"RAGæ¤œç´¢ - {len(all_results)}ä»¶ã®æ–‡æ›¸ã‹ã‚‰çµ±åˆç”Ÿæˆ",
                'score': float(all_results[0][1]) if all_results else 0.1,
                'search_results': search_results,
                'retrieved_docs': self._format_retrieved_docs(all_results)
            }
        
        # Layer 5: ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆï¼ˆRAGå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        generated_response = self._generate_pattern_response(query)
        return {
            'layer': 5,
            'method': 'ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆï¼ˆéRAGï¼‰',
            'confidence': 'low',
            'response': generated_response,
            'source': 'å¤§è°·é¸æ‰‹ã®ç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ç”Ÿæˆï¼ˆRAGæƒ…å ±ãªã—ï¼‰',
            'score': 0.1,
            'search_results': [],
            'retrieved_docs': []
        }
    
    def _format_retrieved_docs(self, results: List[Tuple[int, float]], answer_space: bool = False) -> List[Dict]:
        """æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã®æ•´å½¢"""
        docs = []
        for idx, score in results:
            docs.append({
                'id': int(self.df.iloc[idx]['ID']),
                'question': self.questions[idx],
                'answer': self.answers[idx],
                'score': float(score),
                'search_type': 'å›ç­”ç©ºé–“' if answer_space else 'è³ªå•ç©ºé–“'
            })
        return docs
    
    def _aggregate_multiple_docs(self, results: List[Tuple[int, float]]) -> str:
        """è¤‡æ•°æ–‡æ›¸ã‹ã‚‰ã®æƒ…å ±çµ±åˆï¼ˆRAGã®çœŸä¾¡ï¼‰"""
        if not results:
            return self._generate_pattern_response("ä¸€èˆ¬çš„ãªè³ªå•")
        
        # é–¢é€£ã™ã‚‹è¤‡æ•°ã®å›ç­”ã‚’å–å¾—
        relevant_answers = []
        for idx, score in results:
            if score > 0.1:  # æœ€ä½é™ã®é–¢é€£æ€§
                relevant_answers.append(self.answers[idx])
        
        if not relevant_answers:
            return self._generate_pattern_response("ä¸€èˆ¬çš„ãªè³ªå•")
        
        # è¤‡æ•°å›ç­”ã‹ã‚‰å…±é€šè¦ç´ ã‚’æŠ½å‡ºã—ã¦çµ±åˆ
        combined_keywords = []
        for answer in relevant_answers:
            keywords = re.findall(r'[\w\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]+', answer)
            combined_keywords.extend(keywords)
        
        # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®š
        keyword_freq = Counter(combined_keywords)
        top_keywords = [k for k, v in keyword_freq.most_common(5) if v > 1]
        
        # çµ±åˆå›ç­”ç”Ÿæˆ
        starter = random.choice(self.ohtani_patterns['starters'])
        value = random.choice(top_keywords) if top_keywords else random.choice(self.ohtani_patterns['values'])
        ending = random.choice(self.ohtani_patterns['endings'])
        
        return f"{starter}ã€ãã‚Œã«ã¤ã„ã¦ã¯{value}ã‚’å¤§åˆ‡ã«ã—ãªãŒã‚‰å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚è¤‡æ•°ã®çµŒé¨“ã‹ã‚‰å­¦ã‚“ã ã“ã¨ã‚’æ´»ã‹ã—ã¦ã€ã“ã‚Œã‹ã‚‰ã‚‚æˆé•·ã—ã¦ã„ããŸã„{ending}ã€‚"
    
    def _generate_pattern_response(self, query: str) -> str:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ"""
        starter = random.choice(self.ohtani_patterns['starters'])
        ending = random.choice(self.ohtani_patterns['endings'])
        value = random.choice(self.ohtani_patterns['values'])
        
        templates = [
            f"{starter}ã€{query}ã«ã¤ã„ã¦ã¯ã€{value}ã‚’å¤§åˆ‡ã«{ending}ã€‚",
            f"{query}ã«é–¢ã—ã¦ã¯ã€ã¾ã ã¾ã å­¦ã¶ã“ã¨ãŒå¤šã„{ending}ã€‚",
            f"{starter}ã€{query}ã¨ã„ã†ã®ã¯ã€ã¨ã¦ã‚‚å¤§åˆ‡ãªã“ã¨{ending}ã€‚"
        ]
        
        return random.choice(templates)
    
    def prepare_ai_context(self, query: str, search_results: List[Tuple[int, float]]) -> str:
        """AIç”Ÿæˆç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™"""
        context_parts = []
        
        if search_results:
            context_parts.append("ã€å‚è€ƒã¨ãªã‚‹å¤§è°·é¸æ‰‹ã®éå»ã®ç™ºè¨€ã€‘")
            for i, (idx, score) in enumerate(search_results[:3], 1):
                context_parts.append(f"{i}. Q: {self.questions[idx]}")
                context_parts.append(f"   A: {self.answers[idx]}")
            context_parts.append("")
        
        context_parts.extend([
            "ã€å¤§è°·ç¿”å¹³é¸æ‰‹ã®è©±ã—æ–¹ã®ç‰¹å¾´ã€‘",
            "- è¬™è™šã§ä¸å¯§ãªå£èª¿ï¼ˆã€Œãã†ã§ã™ã­ã€ã€Œã¨æ€ã„ã¾ã™ã€ã‚’ã‚ˆãä½¿ã†ï¼‰",
            "- ãƒãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆã‚„å‘¨ã‚Šã®äººã¸ã®æ„Ÿè¬ã‚’å¿˜ã‚Œãªã„",
            "- æˆé•·ã‚„å­¦ã³ã€ç¶™ç¶šã‚’å¤§åˆ‡ã«ã™ã‚‹å§¿å‹¢",
            "- å‰å‘ãã§èª å®Ÿãªç­”ãˆæ–¹",
            "- é‡çƒã§ã®çµŒé¨“ã‚’äº¤ãˆãªãŒã‚‰ç­”ãˆã‚‹",
            "",
            f"è³ªå•: {query}",
            "",
            "ã‚ãªãŸã¯å¤§è°·ç¿”å¹³é¸æ‰‹ã¨ã—ã¦ã€ä¸Šè¨˜ã®ç‰¹å¾´ã‚’æ´»ã‹ã—ã¦150-250æ–‡å­—ã§è‡ªç„¶ã«å›ç­”ã—ã¦ãã ã•ã„ï¼š",
        ])
        
        return "\n".join(context_parts)

# AI APIå‘¼ã³å‡ºã—é–¢æ•°
def call_gemini_api(prompt: str, api_key: str) -> Optional[str]:
    """Gemini APIå‘¼ã³å‡ºã—"""
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                max_output_tokens=300,
                temperature=0.7
            )
        )
        
        return response.text if hasattr(response, 'text') else None
    except Exception as e:
        return f"Gemini APIã‚¨ãƒ©ãƒ¼: {str(e)}"

def call_openai_api(prompt: str, api_key: str) -> Optional[str]:
    """OpenAI APIå‘¼ã³å‡ºã—"""
    try:
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'gpt-3.5-turbo',
            'messages': [{'role': 'user', 'content': prompt}],
            'max_tokens': 250,
            'temperature': 0.7
        }
        
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            return f"OpenAI APIã‚¨ãƒ©ãƒ¼: {response.status_code}"
    except Exception as e:
        return f"OpenAI APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}"

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    st.title("AIå¤§è°·")
    st.subheader("ğŸš€ é«˜é€ŸRAG + ç”ŸæˆAI ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # æ¤œç´¢è¨­å®š
        search_method = st.selectbox(
            "æ¤œç´¢æ–¹æ³•",
            options=['hybrid', 'tfidf', 'keyword'],
            index=0,
            help="hybrid: è¤‡æ•°æ‰‹æ³•çµ„ã¿åˆã‚ã›ï¼ˆæ¨å¥¨ï¼‰"
        )
        
        threshold = st.slider("æ¤œç´¢é–¾å€¤", 0.1, 0.8, 0.3, 0.05)
        
        st.divider()
        
        # AI APIè¨­å®š
        st.subheader("ğŸ¤– ç”ŸæˆAIè¨­å®š")
        ai_provider = st.selectbox("AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼", ["ãªã—", "Gemini", "OpenAI"])
        
        api_key = ""
        if ai_provider == "Gemini":
            api_key = st.text_input(
                "Gemini API Key",
                type="password",
                value=os.getenv("GEMINI_API_KEY", ""),
                help="Gemini APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
            )
        elif ai_provider == "OpenAI":
            api_key = st.text_input(
                "OpenAI API Key", 
                type="password",
                value=os.getenv("OPENAI_API_KEY", ""),
                help="OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
            )
        
        use_ai = ai_provider != "ãªã—" and bool(api_key)
        
        if use_ai:
            st.success(f"âœ… {ai_provider} API æœ‰åŠ¹")
        else:
            st.info("ğŸ’¡ APIã‚­ãƒ¼æœªè¨­å®š: ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆã‚’ä½¿ç”¨")
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    @st.cache_resource
    def load_rag_system():
        return FastOhtaniRAG('ohtani_rag_final.csv')
    
    with st.spinner("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
        rag = load_rag_system()
    
    st.success(f"âœ… åˆæœŸåŒ–å®Œäº†ï¼ ({len(rag.df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿)")
    
    # ãƒ¡ã‚¤ãƒ³ç”»é¢
    st.markdown("---")
    
    # è³ªå•å…¥åŠ›
    query = st.text_input(
        "ğŸ’¬ å¤§è°·é¸æ‰‹ã«è³ªå•ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: é‡çƒä»¥å¤–ã§èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        help="ã©ã‚“ãªè³ªå•ã§ã‚‚å¤§è°·é¸æ‰‹é¢¨ã«å›ç­”ã—ã¾ã™"
    )
    
    # æ“ä½œãƒœã‚¿ãƒ³
    col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
    
    with col1:
        search_btn = st.button("ğŸ” è³ªå•ã™ã‚‹", type="primary")
    with col2:
        random_btn = st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ")
    with col3:
        if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
            st.rerun()
    with col4:
        show_stats = st.button("ğŸ“Š çµ±è¨ˆ")
    
    # ãƒ©ãƒ³ãƒ€ãƒ è³ªå•
    if random_btn:
        sample_queries = [
            "é‡çƒä»¥å¤–ã®è¶£å‘³ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "ã‚ªãƒ•ã‚·ãƒ¼ã‚ºãƒ³ã®éã”ã—æ–¹ã¯ï¼Ÿ",
            "å¥½ããªé£Ÿã¹ç‰©ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "å°†æ¥ã®ç›®æ¨™ã‚’èã‹ã›ã¦ãã ã•ã„",
            "ãƒ•ã‚¡ãƒ³ã®çš†ã•ã‚“ã¸ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’",
            "å›°é›£ã‚’ä¹—ã‚Šè¶Šãˆã‚‹ç§˜è¨£ã¯ï¼Ÿ",
            "ãƒãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆã¨ã®é–¢ä¿‚ã«ã¤ã„ã¦",
            "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§å¿ƒãŒã‘ã¦ã„ã‚‹ã“ã¨ã¯ï¼Ÿ"
        ]
        query = random.choice(sample_queries)
        search_btn = True
    
    # æ¤œç´¢å®Ÿè¡Œ
    if search_btn and query.strip():
        with st.spinner("ğŸ¤– æ¤œç´¢ãƒ»ç”Ÿæˆä¸­..."):
            start_time = time.time()
            
            # RAGæ¤œç´¢
            result = rag.search(query, method=search_method, threshold=threshold)
            search_time = time.time() - start_time
            
            # AIç”Ÿæˆï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰- ã“ã‚ŒãŒçœŸã®RAGï¼
            ai_response = None
            if use_ai and result.get('search_results'):
                ai_start = time.time()
                # RAG: æ¤œç´¢çµæœã‚’ä½¿ã£ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼·åŒ–
                context = rag.prepare_ai_context(query, result['search_results'])
                
                if ai_provider == "Gemini":
                    ai_response = call_gemini_api(context, api_key)
                elif ai_provider == "OpenAI":
                    ai_response = call_openai_api(context, api_key)
                
                ai_time = time.time() - ai_start
                
                # RAGæˆåŠŸã®è¡¨ç¤º
                if ai_response and not ai_response.startswith("API"):
                    st.info(f"âœ… RAGæˆåŠŸ: {len(result.get('retrieved_docs', []))}ä»¶ã®æ–‡æ›¸ã‹ã‚‰ç”Ÿæˆ ({ai_time:.2f}ç§’)")
            
            # çµæœè¡¨ç¤º
            st.markdown("---")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ãƒ¬ã‚¤ãƒ¤ãƒ¼", f"Layer {result['layer']}")
            with col2:
                confidence_colors = {'high': 'ğŸŸ¢', 'medium': 'ğŸŸ¡', 'low': 'ğŸ”µ'}
                st.metric("ä¿¡é ¼åº¦", f"{confidence_colors[result['confidence']]} {result['confidence']}")
            with col3:
                st.metric("ã‚¹ã‚³ã‚¢", f"{result['score']:.3f}")
            with col4:
                st.metric("æ¤œç´¢æ™‚é–“", f"{search_time:.2f}ç§’")
            
            # å›ç­”è¡¨ç¤º
            if ai_response and not ai_response.startswith("API"):
                st.markdown("### ğŸ¤– RAG + AIç”Ÿæˆå›ç­”")
                st.markdown(f"> {ai_response}")
                
                st.success(f"ğŸ” RAGæ¤œç´¢æˆåŠŸ: {len(result.get('retrieved_docs', []))}ä»¶ã®é–¢é€£æ–‡æ›¸ã‚’ç™ºè¦‹")
                
                with st.expander("ğŸ” RAGæ¤œç´¢è©³ç´°"):
                    st.markdown(f"**æ¤œç´¢æ–¹æ³•:** {result['method']}")
                    st.markdown(f"**å…ƒã®å›ç­”:** {result['response']}")
                    st.markdown(f"**å‡ºå…¸:** {result['source']}")
                    
                    # æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ä¸€è¦§
                    if result.get('retrieved_docs'):
                        st.markdown("**æ¤œç´¢ã•ã‚ŒãŸé–¢é€£æ–‡æ›¸:**")
                        for i, doc in enumerate(result['retrieved_docs'][:3], 1):
                            st.markdown(f"{i}. ã‚¹ã‚³ã‚¢: {doc['score']:.3f}")
                            st.markdown(f"   Q: {doc['question']}")
                            st.markdown(f"   A: {doc['answer'][:100]}...")
            else:
                st.markdown("### ğŸ’¬ RAGæ¤œç´¢å›ç­”")
                st.markdown(f"> {result['response']}")
                
                if result['layer'] <= 4:
                    st.info(f"ğŸ” RAGæ¤œç´¢: {result['method']}ã§é–¢é€£æ–‡æ›¸ã‚’ç™ºè¦‹")
                else:
                    st.warning("âš ï¸ RAGæ¤œç´¢ã§é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãšã€ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆã‚’ä½¿ç”¨")
                
                if ai_response and ai_response.startswith("API"):
                    st.error(f"ğŸš« AIç”Ÿæˆå¤±æ•—: {ai_response}")
                elif not use_ai:
                    st.info("ğŸ’¡ ã‚ˆã‚Šé«˜å“è³ªãªå›ç­”ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§AI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            
            # è©³ç´°æƒ…å ±
            with st.expander("ğŸ“ è©³ç´°æƒ…å ±"):
                st.json({
                    "æ¤œç´¢ãƒ¬ã‚¤ãƒ¤ãƒ¼": result['layer'],
                    "æ¤œç´¢æ–¹æ³•": result['method'], 
                    "ä¿¡é ¼åº¦": result['confidence'],
                    "ã‚¹ã‚³ã‚¢": result['score'],
                    "å‡ºå…¸": result['source'],
                    "æ¤œç´¢æ™‚é–“": f"{search_time:.3f}ç§’"
                })
    
    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    if show_stats:
        st.markdown("---")
        st.markdown("### ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(rag.df))
            st.metric("èªå½™ã‚µã‚¤ã‚º", len(rag.tfidf_search.vocab))
        with col2:
            st.metric("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", len(rag.keyword_search.keyword_index))
            st.metric("ãƒ¡ãƒ¢ãƒªåŠ¹ç‡", "è»½é‡ç‰ˆ")
    
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("---")
    st.markdown("### ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    
    sample_categories = {
        "ğŸƒâ€â™‚ï¸ é‡çƒãƒ»ã‚¹ãƒãƒ¼ãƒ„": [
            "ä»Šã‚·ãƒ¼ã‚ºãƒ³ã®ç›®æ¨™ã¯ï¼Ÿ",
            "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§å¤§åˆ‡ã«ã—ã¦ã„ã‚‹ã“ã¨ã¯ï¼Ÿ",
            "ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã¨ã©ã†å‘ãåˆã£ã¦ã„ã¾ã™ã‹ï¼Ÿ"
        ],
        "ğŸ¯ ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆ": [
            "ã‚ªãƒ•ã®æ—¥ã¯ã©ã†éã”ã—ã¾ã™ã‹ï¼Ÿ", 
            "å¥½ããªé£Ÿã¹ç‰©ã¯ï¼Ÿ",
            "ãƒªãƒ©ãƒƒã‚¯ã‚¹æ–¹æ³•ã¯ï¼Ÿ"
        ],
        "ğŸŒŸ äººç”Ÿè¦³": [
            "å°†æ¥ã®å¤¢ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "å›°é›£ã‚’ä¹—ã‚Šè¶Šãˆã‚‹ç§˜è¨£ã¯ï¼Ÿ",
            "å¤§åˆ‡ã«ã—ã¦ã„ã‚‹ä¾¡å€¤è¦³ã¯ï¼Ÿ"
        ]
    }
    
    for category, questions in sample_categories.items():
        with st.expander(category):
            for i, q in enumerate(questions):
                if st.button(q, key=f"{category}_{i}"):
                    # è³ªå•ã‚’å®Ÿè¡Œ
                    result = rag.search(q, method=search_method, threshold=threshold)
                    st.write(f"**è³ªå•:** {q}")
                    st.write(f"**å›ç­”:** {result['response']}")

if __name__ == "__main__":
    main()